{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audiobook-purchase-prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KooOPszDs4O"
      },
      "source": [
        "# **Audiobook Purchase Prediction**\n",
        "## **Project description**\n",
        "\n",
        "The given data comes from an Audiobook app. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create an algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
        "\n",
        "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertizing to him/her. If we can focus our efforts ONLY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
        "\n",
        "the data is summarized in a .csv file.\n",
        "\n",
        "### **Variables:**\n",
        "\n",
        "*   Customer ID\n",
        "*   Book length in mins_avg (average of all purchases)\n",
        "*   Book length in minutes_sum (sum of all purchases. If equal to Book length in mins_avg, thus customers made 1 purchase only.)\n",
        "*   Price Paid_avg (average of all purchases)\n",
        "*   Price paid_sum (sum of all purchases. If equal to Price Paid_avg, thus customers made 1 purchase only.)\n",
        "*   Review (Boolean variable. 1 = Customer left a review)\n",
        "*   Review (out of 10)\n",
        "*   Total minutes listened\n",
        "*   Completion (from 0 to 1) => Total minutes listened / Total lengths of books the person has purchased\n",
        "*   Support requests (number)\n",
        "*   Last visited minus purchase date (in days) => measures the difference between the last time a person interacted with the platform and the first purchase date. The bigger the difference, the bigger the engagement.\n",
        "\n",
        "So these variables are going to be the inputs of our model (excluding customer ID, as it is completely arbitrary).\n",
        "\n",
        "### **Targets:**\n",
        "\n",
        "The targets are a Boolean variable (0 or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets.<br />\n",
        "In fact, we are predicting if based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. If they do not convert after 6 months, chances are they have gone to a competitor or di not like the Audiobook way of digesting information.\n",
        "\n",
        "**This is a supervised classification problem with two classes: won't buy and will buy, represented by 0s and 1s.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdAyYEKyD2bl"
      },
      "source": [
        "## Create the Deep Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfy8_QZx7BeJ"
      },
      "source": [
        "# import relevant packages\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7vnAo-PD9VX"
      },
      "source": [
        "# import the preprocessed data\n",
        "npz = np.load(\"audiobooks-data-train.npz\")\n",
        "train_inputs, train_targets = npz[\"inputs\"].astype(np.float), npz[\"targets\"].astype(np.int)\n",
        "\n",
        "npz = np.load(\"audiobooks-data-validation.npz\")\n",
        "validation_inputs, validation_targets = npz[\"inputs\"].astype(np.float), npz[\"targets\"].astype(np.int)\n",
        "\n",
        "npz = np.load(\"audiobooks-data-test.npz\")\n",
        "test_inputs, test_targets = npz[\"inputs\"].astype(np.float), npz[\"targets\"].astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w53feC9bFgHu"
      },
      "source": [
        "# define layer sizes\n",
        "input_size = 10\n",
        "output_size = 2\n",
        "hidden_layer_size = 150\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hdpUceuGXmy"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5fmro74GrGn",
        "outputId": "7412c6fe-5f5a-4cda-daa6-f387503809ed"
      },
      "source": [
        "print(f\"Model trained on {train_inputs.shape[0]} sample, validated on {validation_inputs.shape[0]} sample.\")\n",
        "batch_size = 100\n",
        "max_epochs = 100\n",
        "# set an early stopping callback to avoid overfitting (when validation loss starts increasing)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "model.fit(train_inputs, train_targets, batch_size=batch_size, epochs=max_epochs,\n",
        "          callbacks=[early_stopping],\n",
        "          validation_data=(validation_inputs, validation_targets),\n",
        "          verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model trained on 3579 sample, validated on 447 sample.\n",
            "Epoch 1/100\n",
            "36/36 - 1s - loss: 0.5062 - accuracy: 0.7402 - val_loss: 0.3891 - val_accuracy: 0.8233\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 0.4081 - accuracy: 0.7843 - val_loss: 0.3467 - val_accuracy: 0.8479\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.3874 - accuracy: 0.7927 - val_loss: 0.3465 - val_accuracy: 0.8434\n",
            "Epoch 4/100\n",
            "36/36 - 0s - loss: 0.3947 - accuracy: 0.7918 - val_loss: 0.3242 - val_accuracy: 0.8523\n",
            "Epoch 5/100\n",
            "36/36 - 0s - loss: 0.3749 - accuracy: 0.8055 - val_loss: 0.3379 - val_accuracy: 0.8300\n",
            "Epoch 6/100\n",
            "36/36 - 0s - loss: 0.3720 - accuracy: 0.8013 - val_loss: 0.3183 - val_accuracy: 0.8479\n",
            "Epoch 7/100\n",
            "36/36 - 0s - loss: 0.3665 - accuracy: 0.8050 - val_loss: 0.3115 - val_accuracy: 0.8479\n",
            "Epoch 8/100\n",
            "36/36 - 0s - loss: 0.3609 - accuracy: 0.8022 - val_loss: 0.3196 - val_accuracy: 0.8434\n",
            "Epoch 9/100\n",
            "36/36 - 0s - loss: 0.3628 - accuracy: 0.8047 - val_loss: 0.3061 - val_accuracy: 0.8546\n",
            "Epoch 10/100\n",
            "36/36 - 0s - loss: 0.3604 - accuracy: 0.8064 - val_loss: 0.3006 - val_accuracy: 0.8546\n",
            "Epoch 11/100\n",
            "36/36 - 0s - loss: 0.3604 - accuracy: 0.8047 - val_loss: 0.3060 - val_accuracy: 0.8479\n",
            "Epoch 12/100\n",
            "36/36 - 0s - loss: 0.3603 - accuracy: 0.8083 - val_loss: 0.2914 - val_accuracy: 0.8680\n",
            "Epoch 13/100\n",
            "36/36 - 0s - loss: 0.3606 - accuracy: 0.8131 - val_loss: 0.2967 - val_accuracy: 0.8635\n",
            "Epoch 14/100\n",
            "36/36 - 0s - loss: 0.3597 - accuracy: 0.8058 - val_loss: 0.3042 - val_accuracy: 0.8523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e198a2dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rN2MEGXSnuw"
      },
      "source": [
        "On the validation data, the model reaches a score of 85%, which is a good result on a balanced dataset. We should try simpler machine learning model as logistic regression to see if we can reach better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVYDZZAdTSn7"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWHr0ar2TRhi",
        "outputId": "ddd49176-3ad3-4007-e8b8-08e72d1240c7"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
        "print(\"Test loss: {0:.2f}. Test accuracy: {1:.2f}%\".format(test_loss, test_accuracy * 100.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4210 - accuracy: 0.7545\n",
            "Test loss: 0.42. Test accuracy: 75.45%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}